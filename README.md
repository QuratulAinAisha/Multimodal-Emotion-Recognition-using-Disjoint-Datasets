# Multimodal-Emotion-Recognition-using-Disjoint-Datasets
This repository contains the implementation of EmoXFormer, a context-aware multimodal emotion recognition model that leverages disjoint datasets (MELD for text, RAVDESS for audio, FER2013 for vision). It uses a two-stage training strategy with context-gated cross-attention to fuse representations from language, audio, and vision modalities.
